# +
"""OLS."""
# Библиотека для визуализации данных (графики, диаграммы)
import matplotlib.pyplot as plt

# Библиотека для работы с массивами и математическими операциями
import numpy as np

# Класс для выполнения линейной регрессии
from sklearn.linear_model import LinearRegression

# Функция для вычисления среднеквадратичной ошибки (MSE)
from sklearn.metrics import mean_squared_error

# Данные
# Создаём массив x из входных данных (независимая переменная)
# Данные: [2, 3, 5, 7, 9] — это значения x из таблицы
# .reshape(-1, 1) преобразует одномерный массив в двумерный столбец
# (5 строк, 1 столбец),
# так как scikit-learn ожидает двумерный массив для входных данных
x_array = np.array([2, 3, 5, 7, 9]).reshape(-1, 1)

# Создаём массив y из целевых значений (зависимая переменная)
# Данные: [4, 6, 8, 10, 12] — это значения y из таблицы
# y остаётся одномерным, так как это ожидаемый формат для целевых значений
y_array = np.array([4, 6, 8, 10, 12])

# Создаём и обучаем модель линейной регрессии
# LinearRegression() — создаём объект модели линейной регрессии
# Этот класс реализует метод наименьших квадратов (OLS)
# для нахождения оптимальных параметров
model = LinearRegression()

# Метод fit обучает модель на данных
# Он принимает x (матрица признаков) и y (целевые значения
# и вычисляет коэффициенты регрессии
# Внутри fit использует аналитическое решение
# (нормальное уравнение: w = (X^T X)^(-1) X^T y)
model.fit(x_array, y_array)

# Получаем коэффициенты модели
# model.coef_ возвращает коэффициенты (наклон m) для каждого признака
# Так как у нас только один признак (парная регрессия),
# берём первый элемент [0]
# m — это наклон прямой (slope),
# который показывает, насколько y изменяется при изменении x на единицу
m_coef = model.coef_[0]

# model.intercept_ возвращает свободный член (intercept b)
# b — это значение y, когда x = 0, то есть точка пересечения прямой с осью y
b_coef = model.intercept_

# Выводим уравнение прямой в формате y = mx + b
# :.2f форматирует числа до 2 знаков после запятой для удобства чтения
print(f"Уравнение прямой: y = {m_coef:.2f}x + {b_coef:.2f}")

# Предсказания
# Метод predict принимает входные данные x и
# возвращает предсказанные значения y
# y_pred — это значения, которые модель предсказывает для каждого x,
# используя уравнение y = mx + b
y_pred = model.predict(x_array)

# Выводим предсказанные значения для проверки
print("Предсказанные значения:", y_pred)

# Вычисляем среднеквадратичную ошибку (MSE)
# mean_squared_error сравнивает истинные значения y с предсказанными y_pred
# MSE = (1/n) * sum((y_i - y_pred_i)^2), где n — количество наблюдений
# Это мера качества модели: чем меньше MSE,
# тем лучше модель соответствует данным
mse = mean_squared_error(y_array, y_pred)

# Выводим MSE, округлённое до 2 знаков после запятой
print(f"Среднеквадратичная ошибка (MSE): {mse:.2f}")

# Визуализация результатов
# scatter создаёт точечный график: синие точки — это исходные данные (x, y)
# color='blue' задаёт цвет точек, label='Данные' — подпись для легенды
plt.scatter(x_array, y_array, color="blue", label="Данные")

# plot создаёт линию: красная линия — это предсказанные значения (x, y_pred)
# color='red' задаёт цвет линии, label='Линейная регрессия' —
# подпись для легенды
plt.plot(x_array, y_pred, color="red", label="Линейная регрессия")

# Подписываем оси и график
plt.xlabel("x")  # Подпись для оси x
plt.ylabel("y")  # Подпись для оси y
plt.title("Парная линейная регрессия")  # Заголовок графика

# Добавляем легенду, чтобы показать, что означают точки и линия
plt.legend()

# Отображаем график
plt.show()
